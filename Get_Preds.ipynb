{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9884aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Python Libraries\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Dropout, Flatten\n",
    "import os\n",
    "from aksharamukha import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f8b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Segmentation\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path = \"Segmentation/rednoise.jpeg\"\n",
    "\n",
    "img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "hProj = np.sum(img,1)\n",
    "\n",
    "def findPeakRegions(hpp, divider=1.02):\n",
    "    threshold = (np.max(hpp)-np.min(hpp))/divider\n",
    "    peaks = []\n",
    "    peaks_index = []\n",
    "    for i, hppv in enumerate(hpp):\n",
    "        if hppv > threshold:\n",
    "            peaks.append([i, hppv])\n",
    "    return peaks\n",
    "\n",
    "peaks = findPeakRegions(hProj)\n",
    "\n",
    "peaksIndex = np.array(peaks)[:,0].astype(int)\n",
    "\n",
    "segmentedImg = np.copy(img)\n",
    "r,c = segmentedImg.shape\n",
    "\n",
    "for ri in range(r):\n",
    "    if ri in peaksIndex:\n",
    "        segmentedImg[ri, :] = 0     \n",
    "\n",
    "hProjLines = np.sum(segmentedImg,1)\n",
    "hProjLines = np.append(hProjLines,[0,0,0])\n",
    "\n",
    "lines = []\n",
    "lIndexB = []\n",
    "lIndexE = []\n",
    "\n",
    "for ri in range(len(hProjLines)):\n",
    "    if hProjLines[ri]!=0 and hProjLines[ri-1]==0 and hProjLines[ri-2]==0:\n",
    "        lIndexB.append(ri)\n",
    "    if hProjLines[ri]!=0 and hProjLines[ri+1]==0 and hProjLines[ri+2]==0:\n",
    "        lIndexE.append(ri)\n",
    "        \n",
    "for i in range(len(lIndexB)):\n",
    "    lines.append(img[lIndexB[i]:lIndexE[i],:])\n",
    "    cv.imwrite(\"Segmentation/lines/line{}.jpg\".format(i+1),lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "989bab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 36B1-37D5\n",
      "\n",
      " Directory of C:\\Users\\rohai\\OneDrive\\Desktop\\SIH2022-Finals-Vigrah\n",
      "\n",
      "26-08-2022  00:11    <DIR>          .\n",
      "24-08-2022  22:48    <DIR>          ..\n",
      "24-08-2022  20:53    <DIR>          .ipynb_checkpoints\n",
      "26-08-2022  00:11           148,597 Get_Preds.ipynb\n",
      "26-08-2022  00:04    <DIR>          model_files\n",
      "23-08-2022  19:23                23 README.md\n",
      "26-08-2022  00:01    <DIR>          Segmentation\n",
      "               2 File(s)        148,620 bytes\n",
      "               5 Dir(s)  23,991,275,520 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ce10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['a',\n",
    " 'ba',\n",
    " 'be',\n",
    " 'bha',\n",
    " 'bhe',\n",
    " 'bhi',\n",
    " 'bho',\n",
    " 'bhu',\n",
    " 'bhā',\n",
    " 'bhī',\n",
    " 'bhū',\n",
    " 'bi',\n",
    " 'bo',\n",
    " 'bu',\n",
    " 'bā',\n",
    " 'ca',\n",
    " 'ce',\n",
    " 'cha',\n",
    " 'che',\n",
    " 'chi',\n",
    " 'chu',\n",
    " 'chā',\n",
    " 'ci',\n",
    " 'co',\n",
    " 'cu',\n",
    " 'cā',\n",
    " 'cū',\n",
    " 'da',\n",
    " 'de',\n",
    " 'dha',\n",
    " 'dhe',\n",
    " 'dhi',\n",
    " 'dho',\n",
    " 'dhu',\n",
    " 'dhā',\n",
    " 'dhī',\n",
    " 'dhū',\n",
    " 'di',\n",
    " 'do',\n",
    " 'du',\n",
    " 'dā',\n",
    " 'dī',\n",
    " 'e',\n",
    " 'ga',\n",
    " 'ge',\n",
    " 'gha',\n",
    " 'ghe',\n",
    " 'gho',\n",
    " 'ghu',\n",
    " 'ghā',\n",
    " 'gi',\n",
    " 'go',\n",
    " 'gu',\n",
    " 'gā',\n",
    " 'ha',\n",
    " 'he',\n",
    " 'hi',\n",
    " 'ho',\n",
    " 'hu',\n",
    " 'hā',\n",
    " 'hī',\n",
    " 'hū',\n",
    " 'i',\n",
    " 'ja',\n",
    " 'je',\n",
    " 'jha',\n",
    " 'jhi',\n",
    " 'jhā',\n",
    " 'ji',\n",
    " 'jo',\n",
    " 'ju',\n",
    " 'jā',\n",
    " 'jī',\n",
    " 'jū',\n",
    " 'ka',\n",
    " 'ke',\n",
    " 'kha',\n",
    " 'khe',\n",
    " 'khi',\n",
    " 'kho',\n",
    " 'khu',\n",
    " 'khā',\n",
    " 'khī',\n",
    " 'ki',\n",
    " 'ko',\n",
    " 'ku',\n",
    " 'kā',\n",
    " 'kī',\n",
    " 'kū',\n",
    " 'la',\n",
    " 'le',\n",
    " 'li',\n",
    " 'lo',\n",
    " 'lu',\n",
    " 'lā',\n",
    " 'lī',\n",
    " 'lū',\n",
    " 'ma',\n",
    " 'me',\n",
    " 'mi',\n",
    " 'mo',\n",
    " 'mu',\n",
    " 'mā',\n",
    " 'mī',\n",
    " 'mū',\n",
    " 'na',\n",
    " 'ne',\n",
    " 'ni',\n",
    " 'no',\n",
    " 'nu',\n",
    " 'nā',\n",
    " 'nī',\n",
    " 'nū',\n",
    " 'o',\n",
    " 'pa',\n",
    " 'pe',\n",
    " 'pha',\n",
    " 'phe',\n",
    " 'phā',\n",
    " 'pi',\n",
    " 'po',\n",
    " 'pu',\n",
    " 'pā',\n",
    " 'pī',\n",
    " 'ra',\n",
    " 're',\n",
    " 'ri',\n",
    " 'ro',\n",
    " 'ru',\n",
    " 'rā',\n",
    " 'rī',\n",
    " 'rū',\n",
    " 'sa',\n",
    " 'se',\n",
    " 'si',\n",
    " 'so',\n",
    " 'su',\n",
    " 'sā',\n",
    " 'sī',\n",
    " 'sū',\n",
    " 'ta',\n",
    " 'te',\n",
    " 'tha',\n",
    " 'the',\n",
    " 'thi',\n",
    " 'thu',\n",
    " 'thā',\n",
    " 'thī',\n",
    " 'ti',\n",
    " 'to',\n",
    " 'tu',\n",
    " 'tā',\n",
    " 'tī',\n",
    " 'tū',\n",
    " 'u',\n",
    " 'va',\n",
    " 've',\n",
    " 'vi',\n",
    " 'vo',\n",
    " 'vu',\n",
    " 'vā',\n",
    " 'vī',\n",
    " 'vū',\n",
    " 'ya',\n",
    " 'ye',\n",
    " 'yi',\n",
    " 'yo',\n",
    " 'yu',\n",
    " 'yā',\n",
    " 'yī',\n",
    " 'yū',\n",
    " 'ña',\n",
    " 'ñe',\n",
    " 'ño',\n",
    " 'ñā',\n",
    " 'ā',\n",
    " 'śa',\n",
    " 'śi',\n",
    " 'śu',\n",
    " 'śā',\n",
    " 'ḍa',\n",
    " 'ḍe',\n",
    " 'ḍha',\n",
    " 'ḍhe',\n",
    " 'ḍhi',\n",
    " 'ḍhā',\n",
    " 'ḍhī',\n",
    " 'ḍi',\n",
    " 'ḍu',\n",
    " 'ḍā',\n",
    " 'ḍī',\n",
    " 'ṇa',\n",
    " 'ṇe',\n",
    " 'ṇi',\n",
    " 'ṇā',\n",
    " 'ṇī',\n",
    " 'ṣa',\n",
    " 'ṣe',\n",
    " 'ṣi',\n",
    " 'ṣo',\n",
    " 'ṣu',\n",
    " 'ṣā',\n",
    " 'ṭa','ṭe','ṭha','ṭhe','ṭhi','ṭhā','ṭhī','ṭhū','ṭi','ṭu','ṭā','ṭī']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d1f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "brahmi_dict = {'a':'𑀅','ā':'𑀆','i':'𑀇','ī':'𑀈','u':'𑀉','ū':'𑀊','e':'𑀏','ai':'𑀐','o':'𑀑','au':'𑀒','aṃ':'𑀅𑀁',\n",
    "'ka':'𑀓','kā':'𑀓𑀸','ki':'𑀓𑀺','kī':'𑀓𑀻','ku':'𑀓𑀼','kū':'𑀓𑀽','ke':'𑀓𑁂','kai':'𑀓𑁃','ko':'𑀓𑁄','kau':'𑀓𑁅','kaṃ':'𑀓𑀁',\n",
    "'kha':'𑀔​','khā':'𑀔𑀸','khi':'𑀔𑀺','khī':'𑀔𑀻','khu':'𑀔𑀼','khū':'𑀔𑀽','khe':'𑀔𑁂','khai':'𑀔𑁃','kho':'𑀔𑁄','khau':'𑀔𑁅','khaṃ':'𑀔𑀁',\n",
    "'ga':'𑀕​','gā':'𑀕𑀸','gi':'𑀕𑀺','gī':'𑀕𑀻','gu':'𑀕𑀼','gū':'𑀕𑀽','ge':'𑀕𑁂','gai':'𑀕𑁃','go':'𑀕𑁄','gau':'𑀕𑁅','gaṃ':'𑀕𑀁',\n",
    "'gha':'𑀖​','ghā':'𑀖𑀸','ghi':'𑀖𑀺','ghī':'𑀖𑀻','ghu':'𑀖𑀼','ghū':'𑀖𑀽','ghe':'𑀖𑁂','ghai':'𑀖𑁃','gho':'𑀖𑁄','ghau':'𑀖𑁅','ghaṃ':'𑀖𑀁',\n",
    "'ṅa':'𑀗​','ṅā':'𑀗𑀸','ṅi':'𑀗𑀺','ṅī':'𑀗𑀻','ṅu':'𑀗𑀼','ṅū':'𑀗𑀽','ṅe':'𑀗𑁂','ṅai':'𑀗𑁃','ṅo':'𑀗𑁄','ṅau':'𑀗𑁅','ṅaṃ':'𑀗𑀁',\n",
    "'ca':'𑀘​','cā':'𑀘𑀸','ci':'𑀘𑀺','cī':'𑀘𑀻','cu':'𑀘𑀼','cū':'𑀘𑀽','ce':'𑀘𑁂','cai':'𑀘𑁃','co':'𑀘𑁄','cau':'𑀘𑁅','caṃ':'𑀘𑀁',\n",
    "'cha':'𑀙​','chā':'𑀙𑀸','chi':'𑀙𑀺','chī':'𑀙𑀻','chu':'𑀙𑀼','chū':'𑀙𑀽','che':'𑀙𑁂','chai':'𑀙𑁃','cho':'𑀙𑁄','chau':'𑀙𑁅','chaṃ':'𑀙𑀁',\n",
    "'ja':'𑀚','jā':'𑀚𑀸','ji':'𑀚𑀺','jī':'𑀚𑀻','ju':'𑀚𑀼','jū':'𑀚𑀽','je':'𑀚𑁂','jai':'𑀚𑁃','jo':'𑀚𑁄','jau':'𑀚𑁅','jaṃ':'𑀚𑀁',\n",
    "'jha':'𑀛​','jhā':'𑀛𑀸','jhi':'𑀛𑀺','jhī':'𑀛𑀻','jhu':'𑀛𑀼','jhū':'𑀛𑀽','jhe':'𑀛𑁂','jhai':'𑀛𑁃','jho':'𑀛𑁄','jhau':'𑀛𑁅','jhaṃ':'𑀛𑀸𑀁',\n",
    "'ña':'𑀜​','ñā':'𑀜𑀸','ñi':'𑀜𑀺','ñī':'𑀜𑀻','ñu':'𑀜𑀼','ñū':'𑀜𑀽','ñe':'𑀜𑁂','ñai':'𑀜𑁃','ño':'𑀜𑁄','ñau':'𑀜𑁅','ñaṃ':'𑀜𑀁',\n",
    "'ṭa':'𑀝​','ṭā':'𑀝𑀸','ṭi':'𑀝𑀺','ṭī':'𑀝𑀻','ṭu':'𑀝𑀼','ṭū':'𑀝𑀽','ṭe':'𑀝𑁂','ṭai':'𑀝𑁃','ṭo':'𑀝𑁄','ṭau':'𑀝𑁅','ṭaṃ':'𑀝𑀁',\n",
    "'ṭha':'𑀞​','ṭhā':'𑀞𑀸','ṭhi':'𑀞𑀺','ṭhī':'𑀞𑀻','ṭhu':'𑀞𑀼','ṭhū':'𑀞𑀽','ṭhe':'𑀞𑁂','ṭhai':'𑀞𑁃','ṭho':'𑀞𑁄','ṭhau':'𑀞𑁅','ṭhaṃ':'𑀞𑀁',\n",
    "'ḍa':'𑀟​','ḍā':'𑀤𑀸','ḍi':'𑀟𑀺','ḍī':'𑀟𑀻','ḍu':'𑀟𑀼','ḍū':'𑀟𑀽','ḍe':'𑀟𑁂','ḍai':'𑀟𑁃','ḍo':'𑀟𑁄','ḍau':'𑀟𑁅','ḍaṃ':'𑀟𑀁',\n",
    "'ḍha':'𑀠​','ḍhā':'𑀠𑀸','ḍhi':'𑀠𑀺','ḍhī':'𑀠𑀻','ḍhu':'𑀠𑀼','ḍhū':'𑀠𑀽','ḍhe':'𑀠𑁂','ḍhai':'𑀠𑁃','ḍho':'𑀠𑁄','ḍhau':'𑀠𑁅','ḍhaṃ':'𑀠𑀁',\n",
    "'ṇa':'𑀡​','ṇā':'𑀡𑀸','ṇi':'𑀡𑀺','ṇī':'𑀡𑀻','ṇu':'𑀡𑀼','ṇū':'𑀡𑀽','ṇe':'𑀡𑁂','ṇai':'𑀡𑁃','ṇo':'𑀡𑁄','ṇau':'𑀡𑁅','ṇaṃ':'𑀡𑀁',\n",
    "'ta':'𑀢​','tā':'𑀢𑀸','ti':'𑀢𑀺','tī':'𑀢𑀻','tu':'𑀢𑀼','tū':'𑀢𑀽','te':'𑀢𑁂','tai':'𑀢𑁃','to':'𑀢𑁄','tau':'𑀢𑁅','taṃ':'𑀢𑀁',\n",
    "'tha':'𑀣​','thā':'𑀣𑀸','thi':'𑀣𑀺','thī':'𑀣𑀻','thu':'𑀣𑀼','thū':'𑀣𑀽','the':'𑀣𑁂','thai':'𑀣𑁃','tho':'𑀣𑁄','thau':'𑀣𑁅','thaṃ':'𑀣𑀁',\n",
    "'da':'𑀤​','dā':'𑀤𑀸','di':'𑀤𑀺','dī':'𑀤𑀻','du':'𑀤𑀼','dū':'𑀤𑀽','de':'𑀤𑁂','dai':'𑀤𑁃','do':'𑀤𑁄','dau':'𑀤𑁅','daṃ':'𑀤𑀁',\n",
    "'dha':'𑀥​','dhā':'𑀥𑀸','dhi':'𑀥𑀺','dhī':'𑀥𑀻','dhu':'𑀥𑀼','dhū':'𑀥𑀽','dhe':'𑀥𑁂','dhai':'𑀥𑁃','dho':'𑀥𑁄','dhau':'𑀥𑁅','dhaṃ':'𑀥𑀁',\n",
    "'na':'𑀦​','nā':'𑀦𑀸','ni':'𑀦𑀺','nī':'𑀦𑀻','nu':'𑀦𑀼','nū':'𑀦𑀽','ne':'𑀦𑁂','nai':'𑀦𑁃','no':'𑀦𑁄','nau':'𑀦𑁅','naṃ':'𑀦𑀁',\n",
    "'pa':'𑀧​','pā':'𑀧𑀸','pi':'𑀧𑀺','pī':'𑀧𑀻','pu':'𑀧𑀼','pū':'𑀧𑀽','pe':'𑀧𑁂','pai':'𑀧𑁃','po':'𑀧𑁄','pau':'𑀧𑁅','paṃ':'𑀧𑀁',\n",
    "'pha':'𑀨​','phā':'𑀨𑀸','phi':'𑀨𑀺','phī':'𑀨𑀻','phu':'𑀨𑀼','phū':'𑀨𑀽','phe':'𑀨𑁂','phai':'𑀨𑁃','pho':'𑀨𑁄','phau':'𑀨𑁅','phaṃ':'𑀨𑀁',\n",
    "'ba':'𑀩​','bā':'𑀩𑀸','bi':'𑀩𑀺','bī':'𑀩𑀻','bu':'𑀩𑀼','bū':'𑀩𑀽','be':'𑀩𑁂','bai':'𑀩𑁃','bo':'𑀩𑁄','bau':'𑀩𑁅','baṃ':'𑀩𑀁',\n",
    "'bha':'𑀪​','bhā':'𑀪𑀸','bhi':'𑀪𑀺','bhī':'𑀪𑀻','bhu':'𑀪𑀼','bhū':'𑀪𑀽','bhe':'𑀪𑁂','bhai':'𑀪𑁃','bho':'𑀪𑁄','bhau':'𑀪𑁅','bhaṃ':'𑀪𑀁',\n",
    "'ma':'𑀫​','mā':'𑀫𑀸','mi':'𑀫𑀺','mī':'𑀫𑀻','mu':'𑀫𑀼','mū':'𑀫𑀽','me':'𑀫𑁂','mai':'𑀫𑁃','mo':'𑀫𑁄','mau':'𑀫𑁅','maṃ':'𑀫𑀁',\n",
    "'ya':'𑀬​','yā':'𑀬𑀸','yi':'𑀬𑀺','yī':'𑀬𑀻','yu':'𑀬𑀼','yū':'𑀬𑀽','ye':'𑀬𑁂','yai':'𑀬𑁃','yo':'𑀬𑁄','yau':'𑀬𑁅','yaṃ':'𑀬𑀁',\n",
    "'ra':'𑀭​','rā':'𑀭​','ri':'𑀭𑀺','rī':'𑀭𑀻','ru':'𑀭𑀼','rū':'𑀭𑀽','re':'𑀭𑁂','rai':'𑀭𑁃','ro':'𑀭𑁄','rau':'𑀭𑁅','raṃ':'𑀭𑀁',\n",
    "'la':'𑀮​','lā':'𑀮𑀸','li':'𑀮𑀺','lī':'𑀮𑀻','lu':'𑀮𑀼','lū':'𑀮𑀽','le':'𑀮𑁂','lai':'𑀮𑁃','lo':'𑀮𑁄','lau':'𑀮𑁅','laṃ':'𑀮𑀁',\n",
    "'va':'𑀯​','vā':'𑀯𑀸','vi':'𑀯𑀺','vī':'𑀯𑀻','vu':'𑀯𑀼','vū':'𑀯𑀽','ve':'𑀯𑁂','vai':'𑀯𑁃','vo':'𑀯𑁄','vau':'𑀯𑁅','vaṃ':'𑀯𑀁',\n",
    "'śa':'𑀰','śā':'𑀰𑀸','śi':'𑀰𑀺','śī':'𑀰𑀻','śu':'𑀰𑀼','śū':'𑀰𑀽','śe':'𑀰𑁂','śai':'𑀰𑁃','śo':'𑀰𑁄','śau':'𑀰𑁅','śaṃ':'𑀰𑀁',\n",
    "'ṣa':'𑀱​','ṣā':'𑀱𑀸','ṣi':'𑀱𑀺','ṣī':'𑀱𑀻','ṣu':'𑀱𑀼','ṣū':'𑀱𑀽','ṣe':'𑀱𑁂','ṣai':'𑀱𑁃','ṣo':'𑀱𑁄','ṣau':'𑀱𑁅','ṣaṃ':'𑀱𑀁',\n",
    "'sa':'𑀲​','sā':'𑀲𑀸','si':'𑀲𑀺','sī':'𑀲𑀻','su':'𑀲𑀼','sū':'𑀲𑀽','se':'𑀲𑁂','sai':'𑀲𑁃','so':'𑀲𑁄','sau':'𑀲𑁅','saṃ':'𑀲𑀁',\n",
    "'ha':'𑀳​','hā':'𑀳𑀸','hi':'𑀳𑀺','hī':'𑀳𑀻','hu':'𑀳𑀼','hū':'𑀳𑀽','he':'𑀳𑁂','hai':'𑀳𑁃','ho':'𑀳𑁄','hau':'𑀳𑁅','haṃ':'𑀳𑀁'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f096a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation/lines/line1.jpeg\n",
      "Segmentation/characters/char1.png\n",
      "Segmentation/characters/char2.png\n",
      "Segmentation/characters/char3.png\n",
      "Segmentation/characters/char4.png\n"
     ]
    }
   ],
   "source": [
    "# Character Segmentation\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Dropout, Flatten\n",
    "import os\n",
    "from keras.models import model_from_json\n",
    "\n",
    "char_model = model_from_json(open('model_files/model_arch.json').read())\n",
    "\n",
    "char_model.load_weights('model_files/mobilenet_model_weights.h5')\n",
    "\n",
    "# char_model = keras.models.load_model('model_files/best_val_acc_model.h5')\n",
    "\n",
    "line_dir = 'Segmentation/lines/'\n",
    "\n",
    "final_pred = \"\"\n",
    "\n",
    "# def prepare(file):\n",
    "#     IMG_SIZE_X = 100\n",
    "#     IMG_SIZE_Y = 140\n",
    "#     img_array = cv.imread(file, cv.IMREAD_GRAYSCALE)\n",
    "#     new_array = cv.resize(img_array, (IMG_SIZE_X, IMG_SIZE_Y))\n",
    "#     return new_array.reshape(-1, 100, 140, 1)\n",
    "\n",
    "def findPeakRegions(vpp, divider=0.42):\n",
    "    threshold = (np.max(vpp)-np.min(vpp))/divider\n",
    "    peaks = []\n",
    "    peaks_index = []\n",
    "    for i, vppv in enumerate(vpp):\n",
    "        if vppv > threshold:\n",
    "            peaks.append([i, vppv])\n",
    "    return peaks\n",
    "\n",
    "kth_img = 1\n",
    "\n",
    "line_breaks = []\n",
    "\n",
    "for filename in os.listdir(line_dir):\n",
    "    print(line_dir+filename)\n",
    "    img = cv.imread(line_dir + filename, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    vProj = np.sum(img,0)\n",
    "\n",
    "    peaks = findPeakRegions(vProj)\n",
    "    \n",
    "    peaksIndex = np.array(peaks)[:,0].astype(int)\n",
    "\n",
    "    segmentedImg = np.copy(img)\n",
    "    r,c = segmentedImg.shape\n",
    "\n",
    "    for ci in range(c):\n",
    "        if ci in peaksIndex:\n",
    "            segmentedImg[:,ci] = 0     \n",
    "\n",
    "    vProjLines = np.sum(segmentedImg,0)\n",
    "    vProjLines = np.append(vProjLines,[0,0,0])\n",
    "    \n",
    "    chars = []\n",
    "    charsB = []\n",
    "    charsE = []\n",
    "\n",
    "    for ci in range(len(vProjLines)):\n",
    "        if vProjLines[ci]!=0 and vProjLines[ci-1]==0:\n",
    "            charsB.append(ci)\n",
    "        if vProjLines[ci]!=0 and vProjLines[ci+1]==0:\n",
    "            charsE.append(ci)\n",
    "\n",
    "    for i in range(len(charsB)):\n",
    "        chars.append(img[:,charsB[i]:charsE[i]]) \n",
    "        if len(chars[i][0]) != 0 : \n",
    "            cv.imwrite(f\"Segmentation/characters/char{kth_img}.png\",chars[i])\n",
    "            print(f\"Segmentation/characters/char{kth_img}.png\")\n",
    "            kth_img += 1\n",
    "    line_breaks.append(kth_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44e4ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation/characters/char1.png\n",
      "1/1 [==============================] - 0s 290ms/step\n",
      "ni\n",
      "Segmentation/characters/char2.png\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "ji\n",
      "Segmentation/characters/char3.png\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "khe\n",
      "Segmentation/characters/char4.png\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "nā\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Dropout, Flatten\n",
    "import os\n",
    "\n",
    "def prepare(file):\n",
    "    IMG_SIZE = 224\n",
    "    \n",
    "    image = cv.imread(file)\n",
    "    image_resized= cv.resize(image, (224,224))\n",
    "    image=np.expand_dims(image_resized,axis=0)\n",
    "    return image\n",
    "\n",
    "char_dir = 'Segmentation/characters/'\n",
    "\n",
    "num_chars = len([filename for filename in os.listdir(char_dir)])\n",
    "\n",
    "final_pred = \"\"\n",
    "\n",
    "for filename in os.listdir(char_dir):\n",
    "    if i in line_breaks:\n",
    "        final_pred += '\\n '\n",
    "#     img_path = f\"{char_dir}char{i+1}.png\"\n",
    "    img_path = char_dir+filename\n",
    "    print(img_path)\n",
    "    img = prepare(img_path)\n",
    "    pred = char_model.predict([img])\n",
    "    \n",
    "    pred = list(pred[0])\n",
    "    char_type = class_names[pred.index(max(pred))]\n",
    "    print(char_type)\n",
    "    final_pred += brahmi_dict[char_type]\n",
    "    final_pred += \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f3d9203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'𑀅 𑀩\\u200b 𑀳𑀻 𑀳𑀺 𑀓𑀺 𑀓𑀺 𑀩\\u200b 𑀓𑀺 𑀇 𑀡𑀻 𑀭𑀼 𑀭\\u200b 𑀲\\u200b 𑀓𑀺 𑀳𑀺 𑀓𑀺 𑀮\\u200b 𑀓𑀺 𑀓𑀺 𑀓 𑀇 𑀖\\u200b 𑀟\\u200b 𑀭\\u200b 𑀓𑀺 𑀓𑀺 𑀔𑀼 𑀭\\u200b 𑀓𑀺 𑀫𑀽 𑀮\\u200b 𑀳𑀻 𑀮\\u200b 𑀭𑀼 𑀓𑀺 𑀭\\u200b 𑀩𑀼 𑀡𑀺 𑀧𑁂 𑀓𑀺 '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75c90ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transliteration \n",
    "\n",
    "from aksharamukha import transliterate\n",
    "\n",
    "trans_text = transliterate.process('autodetect','Devanagari',final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ede59acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count_vals = Counter(trans_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cdc69442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'𑀕\\u200b': 1100, '': 1})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5e9cca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अ ब​ ही हि कि कि ब​ कि इ णी रु र​ स​ कि हि कि ल​ कि कि क इ घ​ ड​ र​ कि कि खु र​ कि मू ल​ ही ल​ रु कि र​ बु णि पे कि \n"
     ]
    }
   ],
   "source": [
    "print(trans_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7b2ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "char_model = model_from_json(open('model_files/model_arch.json').read())\n",
    "\n",
    "char_model.load_weights('model_files/mobilenet_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d616539c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "ki\n"
     ]
    }
   ],
   "source": [
    "# Single Img\n",
    "\n",
    "img_path = 'Segmentation/char38.png'\n",
    "prepare(img_path)\n",
    "\n",
    "pred = char_model.predict([img])\n",
    "    \n",
    "pred = list(pred[0])\n",
    "char_type = class_names[pred.index(max(pred))]\n",
    "print(char_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
